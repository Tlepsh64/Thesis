# -*- coding: utf-8 -*-
"""FedAvg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XkRZJY_yPV7fd6_mQp9g1TPE3bj_08fL

## Preprocessing
"""

import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
import random
import json

mpl.rcParams['figure.figsize'] = (8, 6)
mpl.rcParams['axes.grid'] = False

TRAIN_BATCH_SIZE = 512
TEST_BATCH_SIZE = 1
TIME_WINDOW_SIZE = 12
EPOCHS = 5

NUMBER_OF_CLIENTS = 50
FL_ROUNDS = 50

def load_data():
    df = pd.read_csv('combined.csv', parse_dates=[1])
    return df

def basic_preprocess_fn(df:pd.DataFrame):
    """ Selects the electricity data with standard tariffs belonging to year 2013 and sorts the dataframe based on 
    DateTime column. """
    
    temp = df[df['DateTime'].dt.strftime('%Y') == '2013'].sort_values(by=['DateTime'])
    temp = temp[temp['stdorToU'] == 'Std']
    temp = temp.drop(axis=1, columns = 'stdorToU')
    return temp

def extract_clients(df:pd.DataFrame):
    """ Extracts client ids and client data from the given dataframe, which is assumed to hold multiple 
    client information."""
    
    client_keys = df['LCLid'].value_counts().keys().tolist()
    client_list = []

    for key in client_keys:
        client_df = df[df['LCLid'] == key]
        client_list.append(client_df)

    return client_keys, client_list

def resample_fn(df:pd.DataFrame):
    """ Resamples the time-series dataframe so that it reflects hourly electricity consumption values. It also does
    basic data processing such as dropping unnecessary columns. """
    
    resample_df = df.astype({"KWH/hh (per half hour) ": float}).drop(columns = ['LCLid']).set_index('DateTime').resample(rule='H').sum()
    resample_df.rename(columns = {'KWH/hh (per half hour) ':'kWh'}, inplace = True)
    return resample_df

def remove_bad_clients(client_keys:list, client_list:list):
    """ Drops the clients with datapoints fewer than 0.9 percent of the client with highest amount of data. """
    
    sizes = [len(e) for e in client_list]
    biggest_client = max(sizes) 
    ratios = [size/biggest_client for size in sizes]
    indices_to_keep = [idx for idx,e in enumerate(ratios) if e>=0.9]
    
    client_keys_selected = [client_keys[index] for index in indices_to_keep]
    client_list_selected = [client_list[index] for index in indices_to_keep]
    
    return client_keys_selected, client_list_selected

def create_client_dict(client_keys, client_data_list):
    """ Matches every client ID with its own electricty consumption data and returns the pairs in a python dictionary. """
    client_dict = {}
    for key, df in zip(client_keys, client_data_list):
        client_dict[key] = df
        
    return client_dict

df = load_data()
#temp = basic_preprocess_fn(df)
client_keys, client_list = extract_clients(df)
client_list_resampled = list(map(resample_fn, client_list))
client_keys_selected, client_list_selected = remove_bad_clients(client_keys, client_list_resampled)
client_dict = create_client_dict(client_keys_selected, client_list_selected)

"""## Creating the Client Class"""

class Client:
    def __init__(self, cid, data, round_id = None):
    
        self.cid = cid
        self.df = data
        self.round_id = round_id
        # self.predictions_hist  = []
        #self.test_metrics_hist = []
        
    def create_model(self):
        """ Initialize an LSTM model."""
        self.model = tf.keras.models.Sequential([
        tf.keras.layers.LSTM(32, dropout=0.1, return_sequences=True),
        tf.keras.layers.LSTM(16, dropout=0.1),
        tf.keras.layers.Dense(1),
        ])
        return self.model
    
    def train_test_val_split(self):
        """ Splits the client data into train, test and validation datasets. """

        df_size = self.df.shape[0]

        self.train = self.df.iloc[:int(df_size * 0.7)]
        self.val = self.df.iloc[int(df_size * 0.7):int(df_size * 0.9)]
        self.test = self.df.iloc[int(df_size * 0.9):]

        return self.train, self.val, self.test
    
    def normalize_fn(self, scaler = MinMaxScaler()):
        """ Normalizes the input arrays with the given scaler. Train array is taken as the reference. """

        self.train_sc = scaler.fit_transform( self.train.values )
        self.val_sc = scaler.transform( self.val.values )
        self.test_sc = scaler.transform( self.test.values )
        self.scaler = scaler
        
        return self.train_sc, self.val_sc, self.test_sc, self.scaler
    
    def create_tensor_datasets(self):
        """ Applies the window function on the scaled train,test and validation arrays. Output will be tf.data datasets."""
        
        def window_fn(input_array, window_size=TIME_WINDOW_SIZE, batch_size=TRAIN_BATCH_SIZE):
            """ Creates time series windows for the given input array and returns them as a batch of tensorflow dataset."""
            dataset = tf.data.Dataset.from_tensor_slices(input_array)
            dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)
            dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
            dataset = dataset.map(lambda window: (window[:-1], window[-1:]), num_parallel_calls=6)
            dataset = dataset.batch(batch_size, num_parallel_calls=6).prefetch(1)

            return dataset
    
        self.train_tf = window_fn(self.train_sc, window_size = TIME_WINDOW_SIZE)
        self.val_tf = window_fn(self.val_sc, window_size = TIME_WINDOW_SIZE)
        self.test_tf = window_fn(self.test_sc, window_size = TIME_WINDOW_SIZE, batch_size=TEST_BATCH_SIZE)
        
        return self.train_tf, self.val_tf, self.test_tf
        
    def train_fn(self, lr = 0.002, opt = tf.keras.optimizers.Adam, epochs = EPOCHS):
        """ Fits the model on train&validation data and returns the corresponding MSE loss values. """
        model = self.model
        train = self.train_tf
        validation = self.val_tf
        
        learning_rate = lr
        optimizer = opt(learning_rate = lr)
        model.compile(loss=tf.keras.losses.MeanSquaredError(),
                  optimizer=optimizer,
                  metrics=["mse"])
        
        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)
        history = model.fit(x=train, epochs=epochs, validation_data=validation, callbacks=[callback], verbose=False)
        # self.train_loss = history.history['loss']
        # self.val_loss = history.history['val_loss']

        return #self.train_loss, self.val_loss
    
    def evaluate(self):
        """ Evaluate the trained local model on local test set, return predicstions and test MSE."""
        new_model = tf.keras.models.Sequential([
        tf.keras.layers.LSTM(32, dropout=0.1, batch_input_shape = (1,12,1), return_sequences=True),
        tf.keras.layers.LSTM(16, dropout=0.1),
        tf.keras.layers.Dense(1),
        ])
        new_model.set_weights(self.model.get_weights())
        
        forecast = self.model.predict(self.test_tf)
        self.predictions = self.scaler.inverse_transform(forecast)
        self.test_metrics = round(mean_squared_error(self.test[TIME_WINDOW_SIZE:], self.predictions), 5)
        self.test_metrics_pct = round(mean_absolute_percentage_error(self.test[TIME_WINDOW_SIZE:], self.predictions), 5)
        
        # self.predictions_hist.append(self.predictions)
        # self.test_metrics_hist.append(self.test_metrics)
        return #self.predictions, self.test_metrics, self.test_metrics_pct
    
    def set_weights(self, avg_weights):
        """ Set the model weights taken from the server."""
        return self.model.set_weights(avg_weights)
    
    def send_weights(self):
        """ Send the local model's weights to server. """
        weights = np.array(self.model.get_weights(), dtype=object)
        return weights

"""## Experiments"""

def instantiate_clients(client_dict):
    """ Takes a dictionary of client, client_data pairs and returns a list of client objects. """
    
    client_objects = []
    for client, data in client_dict.items():
        instance = Client(cid = client, data = data.iloc[:4380]) # Changed to include half a year, Jan to July.
        client_objects.append(instance)
    return client_objects

class Server:
    def __init__(self, number_of_training_rounds):
    
        self.rounds = number_of_training_rounds
        self.current_rnd = 0
        self.mses_overall = []
        self.mapes_overall = []
        
    def federated_training(self, client_instances):
        """ Helper function to orchestrate the model training in clients."""
    
        def initialize_clients():
            for cl in client_instances:
                cl.create_model()
                cl.train_test_val_split()
                cl.normalize_fn()
                cl.create_tensor_datasets()
            return
            
        def train_and_evaluate():
            
            for i in tf.range(self.rounds):
                print('Round', self.current_rnd, 'starting...')
                """ Block that checks the current round and sets the mean weights to all clients before training process."""
                if self.current_rnd != 0:
                    avg = average_weights()
                    for cl in client_instances:
                        cl.set_weights(avg)
                
                """ Block for training the model on clients, getting their weights and appending them to the server's 
                weight list to be used with FedAVG algorithm. """
                self.weights_list = []
                round_test_mses = []
                round_test_mapes = []
                for cl in client_instances:
                    cl.train_fn()
                    cl.evaluate()
                    
                    if cl.round_id == None:
                        cl.round_id = 1
                    else:
                        cl.round_id += 1
                        
                    cl_w = cl.send_weights()
                    self.weights_list.append(cl_w)
                    round_test_mses.append(cl.test_metrics)
                    round_test_mapes.append(cl.test_metrics_pct)
                
                mean_mse = round(np.mean(round_test_mses), 4)
                mean_mape = round(np.mean(round_test_mapes), 4)
                self.mses_overall.append(mean_mse)
                self.mapes_overall.append(mean_mape)
                
                self.current_rnd += 1
            return
    
        def average_weights():
            weights_avg = np.mean( np.array(self.weights_list), axis = 0)
            return weights_avg
    
        initialize_clients()
        train_and_evaluate()
        
        return

client_objects = instantiate_clients(client_dict)
random.seed(10)
client_objects_short = random.sample(client_objects, NUMBER_OF_CLIENTS)

server = Server(number_of_training_rounds = FL_ROUNDS)
server.federated_training(client_objects_short)

"""## Test MSE Scores"""

print('Average test MSE over all clients:', server.mses_overall[-1])
print('Average test MAPE over all clients:', server.mapes_overall[-1])
rounds = [int(i) for i in range(FL_ROUNDS)]

#server.mses_overall
#server.mapes_overall

with open("mses_overall_50rnd_5epoch.json", 'w') as f:
    # indent=2 is not needed but makes the file human-readable 
    # if the data is nested
    json.dump(server.mses_overall, f, indent=2)

with open("mapes_overall_50rnd_5epoch.json", 'w') as f:
    # indent=2 is not needed but makes the file human-readable 
    # if the data is nested
    json.dump(server.mapes_overall, f, indent=2) 

plt.plot(rounds, server.mses_overall)
plt.scatter(x=rounds, y=server.mses_overall, color='r')
plt.xticks(rotation=60, ha='right')
plt.savefig('MSE Curve.png')
plt.close()

plt.plot(rounds, server.mapes_overall)
plt.scatter(x=rounds, y=server.mapes_overall, color='r')
plt.xticks(rotation=60, ha='right')
plt.savefig('MAPE Curve.png')
plt.close()

"""
test_mses = []
test_mapes = []
ids = []
for cl in client_objects_short:
  ids.append(cl.cid)
  test_mses.append(cl.test_metrics)
  test_mapes.append(cl.test_metrics_pct)

print('Average test MSE over all clients:', round(np.mean(test_mses), 3))
print('Average test MAPE over all clients:', round(np.mean(test_mapes), 3))

mses_per_round = [list() for i in range(FL_ROUNDS)]
mapes_per_round = [list() for i in range(FL_ROUNDS)]

for cl in client_objects_short:
  for i in range(FL_ROUNDS):
    mses_per_round[i].append(cl.test_metrics_hist[i])
    mapes_per_round[i].append(cl.test_metrics_pct_hist[i])
    
mse_overall = []
mape_overall = []

for l in mses_per_round:
  mse_overall.append(np.mean(l))
for l in mapes_per_round:
  mape_overall.append(np.mean(l))

plt.plot([int(i) for i in range(20)], mse_overall)
plt.scatter([int(i) for i in range(20)], mse_overall, color='r')
plt.xticks(rotation=60, ha='right')
plt.show()

plt.plot([int(i) for i in range(20)], mape_overall)
plt.scatter(x=[int(i) for i in range(20)], y=mape_overall, color='r')
plt.xticks(rotation=60, ha='right')
plt.show()

plt.scatter(x=ids, y=test_mses)
plt.xticks(rotation=60, ha='right')
plt.show()
plt.savefig('temp_mse.png')

plt.scatter(x=ids, y=test_mapes)
plt.xticks(rotation=60, ha='right')
plt.show()
plt.savefig('temp_mape.png')

##  Showing Predictions_vs_Real for One Client
example_preds = pd.DataFrame(client_objects_short[0].predictions, index = client_objects_short[0].test[12:].index)
example_preds.columns = ['kWh']

plt.plot([i for i in range(len(example_preds))], example_preds.values, label='predicted')
plt.plot([i for i in range(len(example_preds))], client_objects_short[0].test[12:].values, label='real')
plt.xticks(rotation=60, ha='right')
plt.legend()
plt.show()
plt.savefig('Preds_vs_Real_for_one_client.png')
"""
